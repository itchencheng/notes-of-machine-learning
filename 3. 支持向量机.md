## 3. 支持向量机

#### 3.1 本质
支持向量机（Support Vector Machine, SVM）的基本模型是定义在特征空间上的间隔最大的线性分类器。核技巧使之成为实质上的非线性分类器。
SVM的学习策略是间隔最大化，可以形式化为求解凸二次优化问题。也等价于正则化的合页损失函数的最小化问题。
SVM的学习算法是求解凸二次规划的最优化算法。
理解支持向量机可以从简至繁推进：1. 线性可分支持向量机（硬间隔SVM），2. 线性支持向量机（软间隔SVM），3. 非线性支持向量机（软间隔最大化+核技巧）。

#### 3.2 训练数据集
假设给定训练数据集：$T={(x_1,y_1), (x_2, y_2), ..., (x_N, y_N)}$
其中$x \in R^n$，$y\in\{+1, -1\}$，+1表示为正例，-1表示负例。

学习的目标在于在特征空间中寻找一个分离超平面$w \cdot x + b = 0$,将正例和负例正确分开。感知机利用误分类最小的策略。SVM利用间隔最大化的策略。

#### 3.3 线性可分SVM
假设训练数据集线性可分，则存在无穷个分离超平面可将两类数据正确分开。若分类超平面为：$w \cdot x + b = 0$
相应的分类决策函数为：$f(x)=sign(w \cdot x + b)$
这种判别为模型为线性可分SVM。

线性可分SVM的学习策略（寻找超平面的策略）为**间隔最大化**。
那么，何为间隔呢？
对于一个分离超平面$w \cdot x +b = 0$，样本点$x_i$距离超平面的**几何距离**为：
$\gamma_i = \frac{|w \cdot x+b|}{||w||}$。一般来说，点$x_i$距离超平面的远近可以表示分类预测的**确信度**。距离超平面较远的点，其分类预测结果更可信。$w \cdot x_i + b$的符号与类标记$y_i$是否一致表示分类的**正确性**。

间隔就是分类正确性和确信度的一种表达，可分为函数间隔和几何间隔。$|w \cdot x_i + b|$可以相对地表示点$x_i$距离超平面的远近，$y_i(w \cdot x_i +b)$为**函数间隔**。$\frac{y_i(w \cdot x_i + b)}{||w||}$为**几何间隔**。

##### 3.3.1 求解分割超平面的问题建模：间隔最大化
SVM学习的基本想法是求解能够正确划分训练数据集且**几何间隔最大**的分离超平面。
$$max_{wb}   \gamma$$
$$s.t. \frac{y_i(w \cdot x_i + b)}{||w||}>\gamma$$
可以转化为:
$$min_{wb} \frac{1}{2}||w||^2$$

$$s.t. y_i(w \cdot x_i + b)) >= 1$$

##### 3.3.2 支持向量和间隔边界
在线性可分的情况下，训练数据集距离分离超平面最近的样本点成为支持向量（SV）。支持向量是使上面约束式等号成立的点，即：
$$y_i(w \cdot x_i + b) - 1 = 0​$$
对$y_i=+1​$的正例点，支持向量在超平面$H_1:w \cdot x + b =1​$上。
对$y_i=-1​$的负例点，支持向量在超平面$H_2:w \cdot x + b =-1​$上。
$H_1和H_2​$为间隔边界，之间的距离为间隔。
**注:** 决定分离超平面时，只有支持向量起作用。其他点的移动甚至去除都不会影响求解。

##### 3.3.3 问题转化：对偶化